{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnV2WXf3_7bl"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "einTaEzg_7bp"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyR0Lc2e_7bu"
      },
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "\n",
        "contains 70,000 grayscale images in 10 categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyM5qg0G_7bu"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzd3fx2D_7bx"
      },
      "source": [
        "Loading the dataset returns four NumPy arrays:\n",
        "  - train_images, train_labels are the training set\n",
        "  - test_images, test_labels are the test set\n",
        "  \n",
        "images have a 28 x 28 size with pixel values from 0 to 255.\n",
        "Labels are an array of integers, ranging from 0 to 9.\n",
        "Each image is mapped to a single name, we have to define the class names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1gFZlZM_7bx"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRTf_w2K_7bz"
      },
      "source": [
        "# Briefly Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3u7i4qv_7b0"
      },
      "outputs": [],
      "source": [
        "# training set shape:\n",
        "print('Training set image dimension:',str(train_images.shape))\n",
        "print('Training set label dimension:',str(train_labels.shape))\n",
        "\n",
        "# test set shape:\n",
        "print('Test set image dimension:',str(test_images.shape))\n",
        "print('Test set label dimension:',str(test_labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_olYRzfo_7b2"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHxsT__Z_7b3"
      },
      "outputs": [],
      "source": [
        "# Inspect one image\n",
        "plt.figure()\n",
        "plt.imshow(train_images[5000])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQacvq95_7b6"
      },
      "source": [
        "\n",
        "before feeding the data to the neural network model, we want to normalize them in a range of 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBuoCOfh_7b6"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / train_images.max()\n",
        "\n",
        "test_images = test_images / train_images.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuqOEga7_7b8"
      },
      "outputs": [],
      "source": [
        "# Let's show new range\n",
        "plt.figure()\n",
        "plt.imshow(train_images[5000])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbgLap4J_7b_"
      },
      "outputs": [],
      "source": [
        "# Let's show the first 25 images\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH7IQno6_7cC"
      },
      "source": [
        "# Build the model: FILL THE CODE\n",
        "\n",
        "Create a sequential model sequential consisting of \n",
        "\n",
        "- Dense layer with 128 neurons and a relu activation\n",
        "- Dense layer with 10 neurons and softmax activation\n",
        "\n",
        "N.B. the input image must be flattened before being fed to the dense layers!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbicQ7L1_7cC"
      },
      "outputs": [],
      "source": [
        "# Keras sequential model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28),name='input'),\n",
        "    keras.layers.Dense(128, activation='relu',name='fc1'),\n",
        "    keras.layers.Dense(10, activation='softmax',name='output')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jiuFZO6_7cE"
      },
      "outputs": [],
      "source": [
        "# Let's show the architecture of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhDamSDP_7cG"
      },
      "source": [
        "# Compile the model: FILL THE CODE\n",
        "during this step we need to select a few settings:\n",
        "    \n",
        "    - Loss function: measures how the model is accurate \n",
        "      during training (is what we want to minimize)\n",
        "    - Optimizer: how the model is updated based on the data \n",
        "      that it sees and loss function\n",
        "    - Metrics: Used to monitor training and testing steps: \n",
        "      Accuracy fraction of correctly classified images\n",
        "      \n",
        "Use Adam optimizer and a sparse_categorical_crossentropy loss, , use accuracy as a metric\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsfEsHl5_7cG"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyPdxDpm_7cJ"
      },
      "source": [
        "# Train the model: FILL THE CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1glbRrr_7cJ"
      },
      "source": [
        "To train the neural network model, we need to follow these steps:\n",
        "\n",
        "    - 1. Feed training data to the model\n",
        "    \n",
        "    - 2. Model hopefully learns to associate images and labels\n",
        "\n",
        "    - 3. Test model predictions on an \"unseen\" test set and verify accuracy\n",
        "    \n",
        "train the data for 10 epochs using the built-in training loop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTuott-9_7cJ"
      },
      "outputs": [],
      "source": [
        "model.fit(train_images, train_labels, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9BZncfR_7cN"
      },
      "source": [
        "# Evaluate Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQziWbx6_7cO"
      },
      "outputs": [],
      "source": [
        "# Check how model performs on test dataset\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J10BmbaN_7cQ"
      },
      "source": [
        "# Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfoz2WSI_7cQ"
      },
      "outputs": [],
      "source": [
        "# Predictions over test set\n",
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVtK13N3_7cT"
      },
      "outputs": [],
      "source": [
        "# Show result\n",
        "img_idx = 0 # Idx of image\n",
        "print('Model output:',predictions[img_idx])\n",
        "print('Predicted label:', np.argmax(predictions[img_idx]))\n",
        "print('Ground truth label:',test_labels[img_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsAy65-m_7cV"
      },
      "outputs": [],
      "source": [
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    if predicted_label == true_label:\n",
        "        color = 'blue'\n",
        "    else:\n",
        "        color = 'red'\n",
        "\n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "    predictions_array, true_label = predictions_array, true_label[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks(range(10))\n",
        "    plt.yticks([])\n",
        "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "    thisplot[predicted_label].set_color('red')\n",
        "    thisplot[true_label].set_color('blue')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKMr3x3S_7cX"
      },
      "outputs": [],
      "source": [
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "    plot_image(i, predictions[i], test_labels, test_images)\n",
        "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "    plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfro7E9iA9ne"
      },
      "source": [
        "# Feature extraction\n",
        "Let's check how we can extract the output of intermediate layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sypfk8V_7cZ"
      },
      "outputs": [],
      "source": [
        "extractor = keras.Model(inputs=model.inputs,\n",
        "                        outputs = [layer.output for layer in model.layers])\n",
        "features = extractor(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coaPfBKeBVIc"
      },
      "outputs": [],
      "source": [
        "n_layers = len(model.layers)\n",
        "layer_names = ['flatten','fc1','output']\n",
        "num_images = 5\n",
        "for i in range(0,num_images):\n",
        "  plt.figure(figsize=(20, 2))\n",
        "  plt.subplot(1,n_layers+1,1)\n",
        "  plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
        "  for n_l in range(0,n_layers):\n",
        "    plt.subplot(1,n_layers+1,n_l+2)\n",
        "    #print(features[n_l].numpy().shape)\n",
        "    plt.plot(features[n_l].numpy()[i])\n",
        "    plt.title(layer_names[n_l])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lDOvp7BGAeK"
      },
      "source": [
        "#PART 2: Convolutional Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVlnJIBBHsih"
      },
      "source": [
        "#Image Resizing\n",
        "\n",
        "Images are defined as 3D arrays consisting of $M$ rows, $N$ columns and $C$ \n",
        "channels \n",
        "\n",
        "Two main type of images:\n",
        "\n",
        "\n",
        "*   Grayscale images: $C=1$, the channel in this case may sometimes be omitted\n",
        "*   RGB images: $C=3$, each channel corresponds to one color Red, Green or Blue\n",
        "\n",
        "If we consider $I$ as the number of images, there are two ways to represent the images:\n",
        "\n",
        "* Channel last: $I\\times M \\times N \\times C$\n",
        "* Channel first: $I\\times  C \\times M \\times N $ \n",
        "\n",
        "We will use channel last since it is the way that tensorflow prefers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1tW5LeCJCmu"
      },
      "outputs": [],
      "source": [
        "print('Train images size:' +str(train_images.shape))\n",
        "print('Test images size:' +str(test_images.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crs79ZszyQ3t"
      },
      "outputs": [],
      "source": [
        "train_images = tf.expand_dims(train_images,axis=3)\n",
        "test_images = tf.expand_dims(test_images,axis=3)\n",
        "\n",
        "print('Train images size:' +str(train_images.shape))\n",
        "print('Test images size:' +str(test_images.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nuiBfbYznRc"
      },
      "source": [
        "# Build the convolutional Network: FILL THE CODE\n",
        "\n",
        "Create a sequential model sequential consisting of \n",
        "\n",
        "- Conv2D layer with 64 filters and a relu activation\n",
        "- Conv2D layer with 32 filters and a relu activation\n",
        "- Dense layer with 10 neurons and a softmax activation\n",
        "\n",
        "Use kernels of size (4,4), stride of size (2,2) and 'same' for the padding\n",
        "\n",
        "N.B. the input image must be flattened before being fed to the dense layers!\n",
        "N.B. on the first layer input dimension must be specified\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xrBwLAczqni"
      },
      "outputs": [],
      "source": [
        "# Keras sequential model\n",
        "conv_model = keras.Sequential([\n",
        "    keras.layers.Conv2D(filters=64, kernel_size=(4,4), strides=(2,2), activation='relu', input_shape=(28, 28, 1), padding='same', name='conv1'),\n",
        "    keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2), activation='relu', padding='same', name='conv2'),\n",
        "    keras.layers.Flatten(name='flatten'),\n",
        "    keras.layers.Dense(10, activation='softmax',name='output')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sdlpKAKz0S2"
      },
      "outputs": [],
      "source": [
        "# Let's show the architecture of the model\n",
        "conv_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzxDKNTL8VHd"
      },
      "source": [
        "## Compile the model: FILL THE CODE\n",
        "\n",
        "use adam optimizer and sparse_categorical_crossentropy loss, use accuracy as a metric\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL5zvLWxz3f7"
      },
      "outputs": [],
      "source": [
        "conv_model.compile(optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2SvkB7H80Xj"
      },
      "source": [
        "## Train the model: FILL THE CODE\n",
        "\n",
        "Train the convolutional model for 10 epochs using the built-in training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKqnslz14Jex"
      },
      "outputs": [],
      "source": [
        "conv_model.fit(train_images, train_labels, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9hav8coz6vr"
      },
      "outputs": [],
      "source": [
        "# Check how model performs on test dataset\n",
        "test_loss, test_acc = conv_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aEj9cwz88nm"
      },
      "source": [
        "# Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIUXU6BJz-rq"
      },
      "outputs": [],
      "source": [
        "# Predictions over test set\n",
        "predictions = conv_model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRaaVTi70BW7"
      },
      "outputs": [],
      "source": [
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "    plot_image(i, predictions[i], test_labels, test_images[:,:,:,0])\n",
        "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "    plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLjMdgTC9i-m"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GfrcwtI0DS-"
      },
      "outputs": [],
      "source": [
        "extractor = keras.Model(inputs=conv_model.inputs,\n",
        "                        outputs = [layer.output for layer in conv_model.layers])\n",
        "features = extractor(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIswbRex0GEJ"
      },
      "outputs": [],
      "source": [
        "n_layers = len(conv_model.layers)\n",
        "layer_names = ['conv1','conv2','flatten','output']\n",
        "num_images = 5\n",
        "for i in range(0,num_images):\n",
        "  plt.figure(figsize=(20, 2))\n",
        "  plt.subplot(1,n_layers+1,1)\n",
        "  plt.imshow(test_images[i,:,:,0], cmap=plt.cm.binary)\n",
        "  for n_l in range(0,n_layers):\n",
        "    plt.subplot(1,n_layers+1,n_l+2)\n",
        "    if len(features[n_l].numpy().shape) == 4:\n",
        "      plt.imshow(np.mean(features[n_l].numpy()[i,:,:,:], axis=2), cmap=plt.cm.binary)\n",
        "    else:\n",
        "      plt.plot(features[n_l].numpy()[i])\n",
        "    plt.title(layer_names[n_l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khs_VKC0_7cc"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 Fran√ßois Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTEa9bYuPgPn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Ex_I_Introduction_to_Keras_Classification_of_images_of_clothing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
